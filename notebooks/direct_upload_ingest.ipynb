{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ Direct Upload Ingestion - GraphRecall\n",
    "\n",
    "Use this notebook if you already have the processed `.zip` file from a previous run and just need to send it to the backend (e.g., after a runtime disconnect during ingestion).\n",
    "\n",
    "### **Steps:**\n",
    "1.  **Configure API Keys:** Set your backend URL and User ID.\n",
    "2.  **Upload ZIP:** Upload the `processed_book.zip` file directly to this runtime.\n",
    "3.  **Ingest:** The script will unzip it and send the content to your backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 1. Configuration & Setup\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BACKEND_URL = \"https://graphrecall-backend.onrender.com\" # @param {type:\"string\"}\n",
    "USER_ID = \"default_user\" # @param {type:\"string\"}\n",
    "ACCESS_TOKEN = \"\" # @param {type:\"string\"}\n",
    "\n",
    "print(f\"‚úÖ Configured for backend: {BACKEND_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title 2. Upload Processed ZIP File\n",
    "print(\"Please upload your 'processed_book.zip' file...\")\n",
    "uploaded = files.upload()\n",
    "zip_filename = next(iter(uploaded))\n",
    "print(f\"‚úÖ Uploaded: {zip_filename}\")\n",
    "\n",
    "# Unzip\n",
    "extract_path = \"./content/processed_book\"\n",
    "os.makedirs(extract_path, exist_ok=True)\n",
    "with zipfile.ZipFile(zip_filename, \"r\") as zip_ref:\n",
    "    members = [name for name in zip_ref.namelist() if not name.endswith(\"/\")]\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(members)} files to: {extract_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# @title 3. Send to Backend (Batched Ingestion)\n\nimport json\nimport base64\nimport glob\nfrom pathlib import Path\n\nextract_root = Path(extract_path)\n\n# Load content (supports nested folders inside the ZIP)\nmd_candidates = sorted(extract_root.rglob(\"full_text.md\"))\nif not md_candidates:\n    md_candidates = sorted(extract_root.rglob(\"*.md\"))\n\nif not md_candidates:\n    print(\"‚ùå Error: No markdown file found in extracted zip!\")\n    print(\"Extracted files preview:\")\n    for p in sorted(extract_root.rglob(\"*\"))[:50]:\n        if p.is_file():\n            print(f\" - {p}\")\n    raise FileNotFoundError(\"No markdown file found under extracted path\")\n\nmd_path = md_candidates[0]\nwith open(md_path, \"r\", encoding=\"utf-8\") as f:\n    content = f.read()\nprint(f\"üìÑ Loaded markdown from {md_path} ({len(content)} chars)\")\n\n# Helper to enforce base64 string format\ndef file_to_base64(path):\n    with open(path, \"rb\") as f:\n        return base64.b64encode(f.read()).decode('utf-8')\n\n# Find all images in the extracted folder (including nested folders)\nimage_map = {}\nimage_paths = []\nfor ext in (\"png\", \"jpg\", \"jpeg\"):\n    image_paths.extend(glob.glob(f\"{extract_path}/**/*.{ext}\", recursive=True))\n\nprint(f\"üì∑ Found {len(image_paths)} images to process...\")\n\n# Convert images to base64 map\nfor path in image_paths:\n    image_name = os.path.basename(path)\n    image_map[image_name] = file_to_base64(path)\n\nprint(f\"‚úÖ Processed {len(image_map)} images.\")\n\n# Prepare payload\npayload = {\n    \"content\": content,\n    \"images\": image_map,\n    \"title\": os.path.splitext(os.path.basename(zip_filename))[0]\n}\n\n# Send request with longer timeout\nprint(f\"üöÄ Sending ingestion request to {BACKEND_URL}/api/v2/ingest...\")\nstart_time = time.time()\n\ntry:\n    headers = {\n        \"Authorization\": f\"Bearer {ACCESS_TOKEN}\" if ACCESS_TOKEN else None,\n        \"Content-Type\": \"application/json\"\n    }\n    # Remove None headers\n    headers = {k: v for k, v in headers.items() if v}\n    \n    response = requests.post(\n        f\"{BACKEND_URL}/api/v2/ingest\",\n        json=payload,\n        timeout=600,  # 10 minute timeout for large batches\n        headers=headers\n    )\n    \n    response.raise_for_status()\n    result = response.json()\n    \n    elapsed = time.time() - start_time\n    thread_id = result.get('thread_id')\n    print(f\"\\n‚úÖ Ingestion Started! (Took {elapsed:.2f}s to submit)\")\n    print(f\"Thread ID: {thread_id}\")\n    print(f\"Note ID: {result.get('note_id')}\")\n    print(f\"Status: {result.get('status')}\")\n    print(f\"Concepts: {len(result.get('concept_ids', []))}\")\n    print(f\"Flashcards: {len(result.get('flashcard_ids', []))}\")\n    if thread_id:\n        print(f\"Status Check URL: {BACKEND_URL}/api/v2/ingest/{thread_id}/status\")\n    \nexcept requests.exceptions.Timeout:\n    print(\"\\n‚ö†Ô∏è Request Timed Out (Client Side)\")\n    print(\"The backend is likely still processing your request. Check the logs on your backend dashboard.\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"\\n‚ùå Request Failed: {e}\")\n    if hasattr(e, 'response') and e.response:\n        print(f\"Status Code: {e.response.status_code}\")\n        print(f\"Response: {e.response.text}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}