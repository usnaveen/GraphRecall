{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö GraphRecall Book Ingestion Pipeline\n",
    "\n",
    "**All-in-one notebook**: PDF ‚Üí Marker OCR ‚Üí Chunking ‚Üí API Ingestion\n",
    "\n",
    "This notebook is a **backdoor** into GraphRecall. It:\n",
    "1. üìÑ Converts PDF to Markdown using Marker (GPU-accelerated)\n",
    "2. ‚úÇÔ∏è Chunks the markdown with image-aware, heading-preserving chunker\n",
    "3. üß† Sends chunks to your GraphRecall backend API for full processing\n",
    "4. üìä Extracts concepts, builds knowledge graph, generates flashcards & quizzes\n",
    "\n",
    "**Requirements:**\n",
    "- Colab GPU runtime (T4 or better)\n",
    "- Your GraphRecall backend URL and auth token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install marker-pdf requests -q\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration\n",
    "\n",
    "Set your GraphRecall backend URL and authentication details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ‚öôÔ∏è CONFIGURATION - Edit these values\n",
    "# ============================================================\n",
    "\n",
    "# Your GraphRecall backend URL (no trailing slash)\n",
    "BACKEND_URL = \"https://your-graphrecall-backend.com\"  # ‚¨ÖÔ∏è EDIT THIS\n",
    "\n",
    "# Auth: paste your Google OAuth access token here.\n",
    "# Get it from: browser DevTools > Application > Cookies > access_token\n",
    "AUTH_TOKEN = \"\"  # ‚¨ÖÔ∏è PASTE YOUR TOKEN\n",
    "\n",
    "# Book metadata\n",
    "BOOK_TITLE = \"\"  # ‚¨ÖÔ∏è e.g., \"Introduction to Machine Learning\"\n",
    "\n",
    "# Chunking config\n",
    "CHUNK_SIZE = 1400  # Max chars per chunk\n",
    "OVERLAP_RATIO = 0.15  # 15% overlap between chunks\n",
    "\n",
    "# Processing options\n",
    "SKIP_REVIEW = True  # Auto-approve concepts (recommended for books)\n",
    "\n",
    "# ============================================================\n",
    "\n",
    "import requests\n",
    "\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Bearer {AUTH_TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\",\n",
    "}\n",
    "\n",
    "# Verify connection\n",
    "try:\n",
    "    resp = requests.get(f\"{BACKEND_URL}/api/v2/health\", headers=HEADERS, timeout=10)\n",
    "    if resp.status_code == 200:\n",
    "        print(f\"‚úÖ Connected to GraphRecall at {BACKEND_URL}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Backend returned status {resp.status_code}\")\n",
    "        print(f\"   Response: {resp.text[:200]}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Cannot reach backend: {e}\")\n",
    "    print(\"   Make sure your backend is running and the URL is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Upload PDF & Run Marker OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU. Go to Runtime > Change runtime type > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload PDF\n",
    "UPLOAD_METHOD = \"direct\"  # Change to \"drive\" for Google Drive\n",
    "\n",
    "if UPLOAD_METHOD == \"direct\":\n",
    "    from google.colab import files\n",
    "    print(\"üìÅ Select your PDF file...\")\n",
    "    uploaded = files.upload()\n",
    "    PDF_PATH = list(uploaded.keys())[0]\n",
    "    if not BOOK_TITLE:\n",
    "        BOOK_TITLE = PDF_PATH.rsplit('.', 1)[0]\n",
    "    print(f\"‚úÖ Uploaded: {PDF_PATH}\")\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    PDF_PATH = \"/content/drive/MyDrive/Your_Book.pdf\"  # ‚¨ÖÔ∏è EDIT THIS\n",
    "    if not BOOK_TITLE:\n",
    "        BOOK_TITLE = PDF_PATH.rsplit('/', 1)[-1].rsplit('.', 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from marker.converters.pdf import PdfConverter\n",
    "from marker.models import create_model_dict\n",
    "from marker.output import text_from_rendered\n",
    "\n",
    "pdf_path = Path(PDF_PATH)\n",
    "print(f\"üìñ Processing: {pdf_path.name} ({pdf_path.stat().st_size / 1024 / 1024:.1f} MB)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"üîß Loading OCR models...\")\n",
    "model_dict = create_model_dict()\n",
    "converter = PdfConverter(artifact_dict=model_dict)\n",
    "\n",
    "print(\"üìù Extracting text and images...\")\n",
    "rendered = converter(str(pdf_path))\n",
    "markdown_text, _, images = text_from_rendered(rendered)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úÖ OCR complete in {elapsed:.1f}s\")\n",
    "print(f\"   Text: {len(markdown_text):,} chars | Images: {len(images) if images else 0}\")\n",
    "\n",
    "# Save images locally\n",
    "output_dir = Path(f\"/content/{pdf_path.stem}_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "images_dir = output_dir / \"images\"\n",
    "images_dir.mkdir(exist_ok=True)\n",
    "\n",
    "if images:\n",
    "    for img_name, img_data in images.items():\n",
    "        img_path = images_dir / img_name\n",
    "        if hasattr(img_data, 'save'):\n",
    "            img_data.save(str(img_path))\n",
    "        elif isinstance(img_data, bytes):\n",
    "            img_path.write_bytes(img_data)\n",
    "    print(f\"   Saved {len(images)} images to {images_dir}\")\n",
    "\n",
    "# Save markdown\n",
    "md_path = output_dir / f\"{pdf_path.stem}.md\"\n",
    "md_path.write_text(markdown_text, encoding=\"utf-8\")\n",
    "print(f\"   Saved markdown to {md_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Chunk the Book\n",
    "\n",
    "Uses GraphRecall's image-aware BookChunker: detects figures, preserves headings, smart overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional\n",
    "\n",
    "# ============================================================\n",
    "# BookChunker (copied from backend/services/book_chunker.py)\n",
    "# ============================================================\n",
    "\n",
    "IMAGE_PATTERN = re.compile(r\"!\\[[^\\]]*\\]\\((?P<path>[^)]+)\\)\")\n",
    "CAPTION_PATTERN = re.compile(r\"^(Figure|Fig\\.?|FIGURE)\\s+[\\w\\.\\-]+[:\\.']?\\s*(?P<caption>.+)$\")\n",
    "\n",
    "@dataclass\n",
    "class ImageInfo:\n",
    "    filename: str\n",
    "    caption: Optional[str]\n",
    "    page: Optional[int]\n",
    "    url: Optional[str] = None\n",
    "\n",
    "@dataclass\n",
    "class Chunk:\n",
    "    index: int\n",
    "    text: str\n",
    "    images: List[ImageInfo] = field(default_factory=list)\n",
    "    headings: List[str] = field(default_factory=list)\n",
    "\n",
    "class BookChunker:\n",
    "    def __init__(self, max_chars=1400, overlap_ratio=0.15):\n",
    "        self.max_chars = max_chars\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "\n",
    "    def chunk_markdown(self, md_path, images_dir):\n",
    "        lines = md_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "        return self._chunk_lines(lines, images_dir)\n",
    "\n",
    "    def _chunk_lines(self, lines, images_dir):\n",
    "        units = []\n",
    "        current_para = []\n",
    "        heading_stack = []\n",
    "\n",
    "        def flush_para():\n",
    "            if current_para:\n",
    "                text = \"\\n\".join(current_para).strip()\n",
    "                if text:\n",
    "                    units.append({\"type\": \"text\", \"text\": text, \"headings\": heading_stack.copy()})\n",
    "                current_para.clear()\n",
    "\n",
    "        for idx, line in enumerate(lines):\n",
    "            stripped = line.strip()\n",
    "            if stripped.startswith(\"#\"):\n",
    "                flush_para()\n",
    "                level = len(stripped) - len(stripped.lstrip(\"#\"))\n",
    "                heading_text = stripped.lstrip(\"#\").strip()\n",
    "                if level <= len(heading_stack):\n",
    "                    heading_stack[:] = heading_stack[:level - 1]\n",
    "                heading_stack.append(heading_text)\n",
    "                units.append({\"type\": \"text\", \"text\": heading_text, \"headings\": heading_stack.copy()})\n",
    "                continue\n",
    "\n",
    "            image_match = IMAGE_PATTERN.search(stripped)\n",
    "            if image_match:\n",
    "                flush_para()\n",
    "                raw_path = image_match.group(\"path\").strip()\n",
    "                normalized_name, page_num = self._normalize_filename(raw_path, images_dir)\n",
    "                caption = self._find_caption(lines, idx)\n",
    "                image_info = ImageInfo(filename=normalized_name, caption=caption, page=page_num)\n",
    "                placeholder = caption or f\"Image {normalized_name}\"\n",
    "                units.append({\"type\": \"figure\", \"text\": f\"[Figure] {placeholder}\", \"images\": [image_info], \"headings\": heading_stack.copy()})\n",
    "                continue\n",
    "\n",
    "            if stripped == \"\":\n",
    "                flush_para()\n",
    "            else:\n",
    "                current_para.append(stripped)\n",
    "        flush_para()\n",
    "\n",
    "        chunks = []\n",
    "        buf_units, buf_images, buf_headings = [], [], []\n",
    "        current_len = 0\n",
    "\n",
    "        def flush_chunk():\n",
    "            nonlocal buf_units, buf_images, buf_headings, current_len\n",
    "            if not buf_units: return\n",
    "            text = \"\\n\\n\".join(u[\"text\"] for u in buf_units).strip()\n",
    "            chunks.append(Chunk(index=len(chunks), text=text, images=list(buf_images), headings=list(buf_headings)))\n",
    "            overlap_chars = int(self.max_chars * self.overlap_ratio)\n",
    "            carry_units, carry_len = [], 0\n",
    "            for u in reversed(buf_units):\n",
    "                u_len = len(u.get(\"text\", \"\"))\n",
    "                if carry_len + u_len > overlap_chars: break\n",
    "                if u.get(\"type\") == \"figure\": break\n",
    "                carry_units.insert(0, u)\n",
    "                carry_len += u_len + 2\n",
    "            buf_units[:] = carry_units\n",
    "            buf_images[:] = []\n",
    "            buf_headings[:] = list(buf_headings)\n",
    "            current_len = carry_len\n",
    "\n",
    "        for unit in units:\n",
    "            unit_text = unit.get(\"text\", \"\").strip()\n",
    "            if not unit_text: continue\n",
    "            unit_len = len(unit_text)\n",
    "            if current_len + unit_len > self.max_chars and buf_units:\n",
    "                flush_chunk()\n",
    "            buf_units.append(unit)\n",
    "            buf_images.extend(unit.get(\"images\", []))\n",
    "            if unit.get(\"headings\"): buf_headings[:] = unit[\"headings\"]\n",
    "            current_len += unit_len + 2\n",
    "        flush_chunk()\n",
    "        return chunks\n",
    "\n",
    "    def _normalize_filename(self, raw_path, images_dir):\n",
    "        raw_name = Path(raw_path).name\n",
    "        stem = Path(raw_name).stem\n",
    "        for ext in (\".png\", \".jpeg\", \".jpg\"):\n",
    "            candidate = images_dir / f\"{stem}{ext}\"\n",
    "            if candidate.exists():\n",
    "                return candidate.name, self._extract_page(stem)\n",
    "        return raw_name, self._extract_page(stem)\n",
    "\n",
    "    def _extract_page(self, stem):\n",
    "        match = re.search(r\"_page_(\\d+)_\", stem)\n",
    "        return int(match.group(1)) if match else None\n",
    "\n",
    "    def _find_caption(self, lines, idx):\n",
    "        for offset in [-2, -1, 1, 2]:\n",
    "            pos = idx + offset\n",
    "            if 0 <= pos < len(lines):\n",
    "                match = CAPTION_PATTERN.match(lines[pos].strip())\n",
    "                if match: return match.group(\"caption\").strip()\n",
    "        return None\n",
    "\n",
    "# Run chunker\n",
    "chunker = BookChunker(max_chars=CHUNK_SIZE, overlap_ratio=OVERLAP_RATIO)\n",
    "chunks = chunker.chunk_markdown(md_path, images_dir)\n",
    "\n",
    "total_images = sum(len(c.images) for c in chunks)\n",
    "print(f\"‚úÖ Chunked into {len(chunks)} chunks\")\n",
    "print(f\"   Avg chunk size: {sum(len(c.text) for c in chunks) // max(len(chunks), 1)} chars\")\n",
    "print(f\"   Chunks with images: {sum(1 for c in chunks if c.images)}\")\n",
    "print(f\"   Total image references: {total_images}\")\n",
    "\n",
    "# Preview first chunk\n",
    "if chunks:\n",
    "    print(f\"\\n--- Chunk 0 Preview ---\")\n",
    "    print(chunks[0].text[:300])\n",
    "    if chunks[0].images:\n",
    "        print(f\"  Images: {[i.filename for i in chunks[0].images]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Send to GraphRecall Backend\n",
    "\n",
    "Sends the chunked book through the `/api/v2/ingest` endpoint.\n",
    "The backend will: embed, extract concepts, build graph, generate flashcards & quizzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport time\n\n# Reassemble chunks into a single markdown with page markers for the backend\n# The backend's BookChunker will re-chunk, but we include heading context\nfull_markdown = markdown_text\n\nprint(f\"üöÄ Sending '{BOOK_TITLE}' to GraphRecall...\")\nprint(f\"   Content length: {len(full_markdown):,} chars\")\nprint(f\"   Backend: {BACKEND_URL}\")\nprint(\"-\" * 50)\n\n# Split into manageable batches if very large (>50k chars)\nMAX_BATCH_SIZE = 50000\n\nif len(full_markdown) <= MAX_BATCH_SIZE:\n    # Single ingestion call\n    payload = {\n        \"content\": full_markdown,\n        \"title\": BOOK_TITLE,\n        \"skip_review\": SKIP_REVIEW,\n        \"resource_type\": \"book\",\n    }\n    resp = requests.post(\n        f\"{BACKEND_URL}/api/v2/ingest\",\n        headers=HEADERS,\n        json=payload,\n        timeout=300,\n    )\n    if resp.status_code == 200:\n        result = resp.json()\n        print(f\"‚úÖ Ingestion complete!\")\n        print(f\"   Note ID: {result.get('note_id')}\")\n        print(f\"   Concepts: {len(result.get('concepts', []))}\")\n        print(f\"   Flashcards: {len(result.get('flashcard_ids', []))}\")\n        print(f\"   Status: {result.get('status')}\")\n    else:\n        print(f\"‚ùå Failed: {resp.status_code}\")\n        print(resp.text[:500])\nelse:\n    # Batch ingestion for large books\n    # Split by chunks and send in batches\n    batch_size = 15  # chunks per batch\n    total_concepts = 0\n    total_flashcards = 0\n    note_ids = []\n\n    for i in range(0, len(chunks), batch_size):\n        batch = chunks[i:i + batch_size]\n        batch_text = \"\\n\\n\".join(c.text for c in batch)\n        batch_num = i // batch_size + 1\n        total_batches = (len(chunks) + batch_size - 1) // batch_size\n\n        print(f\"  Batch {batch_num}/{total_batches} ({len(batch)} chunks, {len(batch_text):,} chars)...\")\n\n        payload = {\n            \"content\": batch_text,\n            \"title\": f\"{BOOK_TITLE} (Part {batch_num})\",\n            \"skip_review\": SKIP_REVIEW,\n            \"resource_type\": \"book\",\n        }\n        try:\n            resp = requests.post(\n                f\"{BACKEND_URL}/api/v2/ingest\",\n                headers=HEADERS,\n                json=payload,\n                timeout=300,\n            )\n            if resp.status_code == 200:\n                result = resp.json()\n                nc = len(result.get('concepts', []))\n                nf = len(result.get('flashcard_ids', []))\n                total_concepts += nc\n                total_flashcards += nf\n                note_ids.append(result.get('note_id'))\n                print(f\"    ‚úÖ +{nc} concepts, +{nf} flashcards\")\n            else:\n                print(f\"    ‚ùå Failed: {resp.status_code} - {resp.text[:200]}\")\n        except Exception as e:\n            print(f\"    ‚ùå Error: {e}\")\n\n        time.sleep(2)  # Rate limiting\n\n    print(f\"\\n‚úÖ Book ingestion complete!\")\n    print(f\"   Total concepts: {total_concepts}\")\n    print(f\"   Total flashcards: {total_flashcards}\")\n    print(f\"   Note parts: {len(note_ids)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Trigger Community Recomputation\n",
    "\n",
    "After ingesting a book, recompute communities to include new concepts in the knowledge graph hierarchy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Recomputing communities...\")\n",
    "try:\n",
    "    resp = requests.post(\n",
    "        f\"{BACKEND_URL}/api/graph3d/communities/recompute\",\n",
    "        headers=HEADERS,\n",
    "        timeout=120,\n",
    "    )\n",
    "    if resp.status_code == 200:\n",
    "        result = resp.json()\n",
    "        print(f\"‚úÖ Communities recomputed: {result.get('num_communities', '?')} communities\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Community recompute returned {resp.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Failed: {e} (not critical, communities will be computed on next app load)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Done!\n",
    "\n",
    "Your book is now in GraphRecall. Open the app to:\n",
    "- üåê See new concepts in the Knowledge Graph\n",
    "- üìö Browse the book in Library\n",
    "- üß† Study with auto-generated flashcards & quizzes\n",
    "- üí¨ Ask the AI assistant about the book's content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Download the markdown + images locally too\n",
    "import shutil\n",
    "from google.colab import files\n",
    "\n",
    "zip_path = f\"/content/{pdf_path.stem}_extracted\"\n",
    "shutil.make_archive(zip_path, 'zip', output_dir)\n",
    "print(f\"üì¶ Download backup: {zip_path}.zip\")\n",
    "files.download(f\"{zip_path}.zip\")"
   ]
  }
 ]
}